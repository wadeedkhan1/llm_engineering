{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06cf3063-9f3e-4551-a0d5-f08d9cabb927",
   "metadata": {},
   "source": [
    "# Welcome to Week 2!\n",
    "\n",
    "## Frontier Model APIs\n",
    "\n",
    "In Week 1, we used multiple Frontier LLMs through their Chat UI, and we connected with the OpenAI's API.\n",
    "\n",
    "Today we'll connect with the APIs for Anthropic and Google, as well as OpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b268b6e-0ba4-461e-af86-74a41f4d681f",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Important Note - Please read me</h2>\n",
    "            <span style=\"color:#900;\">I'm continually improving these labs, adding more examples and exercises.\n",
    "            At the start of each week, it's worth checking you have the latest code.<br/>\n",
    "            First do a <a href=\"https://chatgpt.com/share/6734e705-3270-8012-a074-421661af6ba9\">git pull and merge your changes as needed</a>. Any problems? Try asking ChatGPT to clarify how to merge - or contact me!<br/><br/>\n",
    "            After you've pulled the code, from the llm_engineering directory, in an Anaconda prompt (PC) or Terminal (Mac), run:<br/>\n",
    "            <code>conda env update --f environment.yml</code><br/>\n",
    "            Or if you used virtualenv rather than Anaconda, then run this from your activated environment in a Powershell (PC) or Terminal (Mac):<br/>\n",
    "            <code>pip install -r requirements.txt</code>\n",
    "            <br/>Then restart the kernel (Kernel menu >> Restart Kernel and Clear Outputs Of All Cells) to pick up the changes.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Reminder about the resources page</h2>\n",
    "            <span style=\"color:#f71;\">Here's a link to resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85cfe275-4705-4d30-abea-643fbddf1db0",
   "metadata": {},
   "source": [
    "## Setting up your keys\n",
    "\n",
    "If you haven't done so already, you could now create API keys for Anthropic and Google in addition to OpenAI.\n",
    "\n",
    "**Please note:** if you'd prefer to avoid extra API costs, feel free to skip setting up Anthopic and Google! You can see me do it, and focus on OpenAI for the course. You could also substitute Anthropic and/or Google for Ollama, using the exercise you did in week 1.\n",
    "\n",
    "For OpenAI, visit https://openai.com/api/  \n",
    "For Anthropic, visit https://console.anthropic.com/  \n",
    "For Google, visit https://ai.google.dev/gemini-api  \n",
    "\n",
    "### Also - adding DeepSeek if you wish\n",
    "\n",
    "Optionally, if you'd like to also use DeepSeek, create an account [here](https://platform.deepseek.com/), create a key [here](https://platform.deepseek.com/api_keys) and top up with at least the minimum $2 [here](https://platform.deepseek.com/top_up).\n",
    "\n",
    "### Adding API keys to your .env file\n",
    "\n",
    "When you get your API keys, you need to set them as environment variables by adding them to your `.env` file.\n",
    "\n",
    "```\n",
    "OPENAI_API_KEY=xxxx\n",
    "ANTHROPIC_API_KEY=xxxx\n",
    "GOOGLE_API_KEY=xxxx\n",
    "DEEPSEEK_API_KEY=xxxx\n",
    "```\n",
    "\n",
    "Afterwards, you may need to restart the Jupyter Lab Kernel (the Python process that sits behind this notebook) via the Kernel menu, and then rerun the cells from the top."
   ]
  },
  {
   "cell_type": "code",
   "id": "de23bb9e-37c5-4377-9a82-d7b6c648eeb6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:28.054967Z",
     "start_time": "2025-11-01T13:32:28.042018Z"
    }
   },
   "source": [
    "# imports\n",
    "\n",
    "# import os\n",
    "# from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "# import anthropic\n",
    "from IPython.display import Markdown, display, update_display"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "f0a8ab2b-6134-4104-a1bc-c3cd7ea4cd36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:05.785685Z",
     "start_time": "2025-11-01T13:32:02.848183Z"
    }
   },
   "source": [
    "# import for google\n",
    "# in rare cases, this seems to give an error on some systems, or even crashes the kernel\n",
    "# If this happens to you, simply ignore this cell - I give an alternative approach for using Gemini later\n",
    "\n",
    "import google.generativeai"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\lenovo\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "1179b4c5-cd1f-4131-a876-4c9f3f38d2ba",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:09:55.626296Z",
     "start_time": "2025-11-01T15:09:55.310524Z"
    }
   },
   "source": [
    "# Load environment variables in a file called .env\n",
    "# Print the key prefixes to help with any debugging\n",
    "\n",
    "# load_dotenv(override=True)\n",
    "# openai_api_key = os.getenv('OPENAI_API_KEY')\n",
    "# anthropic_api_key = os.getenv('ANTHROPIC_API_KEY')\n",
    "# Initialize\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "google_api_key = os.getenv('GOOGLE_API_KEY')\n",
    "\n",
    "# if openai_api_key:\n",
    "#     print(f\"OpenAI API Key exists and begins {openai_api_key[:8]}\")\n",
    "# else:\n",
    "#     print(\"OpenAI API Key not set\")\n",
    "#\n",
    "# if anthropic_api_key:\n",
    "#     print(f\"Anthropic API Key exists and begins {anthropic_api_key[:7]}\")\n",
    "# else:\n",
    "#     print(\"Anthropic API Key not set\")\n",
    "\n",
    "if google_api_key:\n",
    "    print(f\"Google API Key exists and begins {google_api_key[:8]}\")\n",
    "else:\n",
    "    print(\"Google API Key not set\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Google API Key exists and begins AIzaSyBO\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "id": "797fe7b0-ad43-42d2-acf0-e4f309b112f0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:05.972526Z",
     "start_time": "2025-11-01T13:32:05.963030Z"
    }
   },
   "source": [
    "# # Connect to OpenAI, Anthropic\n",
    "#\n",
    "# openai = OpenAI()\n",
    "#\n",
    "# claude = anthropic.Anthropic()"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "code",
   "id": "425ed580-808d-429b-85b0-6cba50ca1d0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:05.995610Z",
     "start_time": "2025-11-01T13:32:05.991087Z"
    }
   },
   "source": [
    "# This is the set up code for Gemini\n",
    "# Having problems with Google Gemini setup? Then just ignore this cell; when we use Gemini, I'll give you an alternative that bypasses this library altogether\n",
    "\n",
    "google.generativeai.configure(api_key =google_api_key)"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "42f77b59-2fb1-462a-b90d-78994e4cef33",
   "metadata": {},
   "source": [
    "## Asking LLMs to tell a joke\n",
    "\n",
    "It turns out that LLMs don't do a great job of telling jokes! Let's compare a few models.\n",
    "Later we will be putting LLMs to better use!\n",
    "\n",
    "### What information is included in the API\n",
    "\n",
    "Typically we'll pass to the API:\n",
    "- The name of the model that should be used\n",
    "- A system message that gives overall context for the role the LLM is playing\n",
    "- A user message that provides the actual prompt\n",
    "\n",
    "There are other parameters that can be used, including **temperature** which is typically between 0 and 1; higher for more random output; lower for more focused and deterministic."
   ]
  },
  {
   "cell_type": "code",
   "id": "378a0296-59a2-45c6-82eb-941344d3eeff",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.007449Z",
     "start_time": "2025-11-01T13:32:06.002354Z"
    }
   },
   "source": [
    "system_message = \"You are an assistant that is great at telling jokes\"\n",
    "user_prompt = \"Tell a light-hearted joke for an audience of Data Scientists\""
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "f4d56a0f-2a3d-484d-9344-0efa6862aff4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.017851Z",
     "start_time": "2025-11-01T13:32:06.007449Z"
    }
   },
   "source": [
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": system_message},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "  ]"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "3b3879b6-9a55-4fed-a18c-1ea2edfaf397",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.066937Z",
     "start_time": "2025-11-01T13:32:06.061541Z"
    }
   },
   "source": [
    "# # GPT-4o-mini\n",
    "#\n",
    "# completion = openai.chat.completions.create(model='gpt-4o-mini', messages=prompts)\n",
    "# print(completion.choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "3d2d6beb-1b81-466f-8ed1-40bf51e7adbf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.078763Z",
     "start_time": "2025-11-01T13:32:06.073648Z"
    }
   },
   "source": [
    "# # GPT-4.1-mini\n",
    "# # Temperature setting controls creativity\n",
    "#\n",
    "# completion = openai.chat.completions.create(\n",
    "#     model='gpt-4.1-mini',\n",
    "#     messages=prompts,\n",
    "#     temperature=0.7\n",
    "# )\n",
    "# print(completion.choices[0].message.content)"
   ],
   "outputs": [],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "12d2a549-9d6e-4ea0-9c3e-b96a39e9959e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T15:11:34.093833Z",
     "start_time": "2025-11-01T15:11:17.521981Z"
    }
   },
   "source": [
    "gemini = OpenAI(\n",
    "    api_key=google_api_key,\n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "completion = gemini.chat.completions.create(\n",
    "    model='gemini-2.5-flash',\n",
    "    messages=prompts\n",
    ")\n",
    "print(completion.choices[0].message.content)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the statistician?\n",
      "\n",
      "Because she found out their correlation wasn't causation!\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "dd1e17bc-cd46-4c23-b639-0c7b748e6c5a",
   "metadata": {},
   "source": [
    "## A rare problem with Claude streaming on some Windows boxes\n",
    "\n",
    "2 students have noticed a strange thing happening with Claude's streaming into Jupyter Lab's output -- it sometimes seems to swallow up parts of the response.\n",
    "\n",
    "To fix this, replace the code:\n",
    "\n",
    "`print(text, end=\"\", flush=True)`\n",
    "\n",
    "with this:\n",
    "\n",
    "`clean_text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \")`  \n",
    "`print(clean_text, end=\"\", flush=True)`\n",
    "\n",
    "And it should work fine!"
   ]
  },
  {
   "cell_type": "code",
   "id": "6df48ce5-70f8-4643-9a50-b0b5bfdb66ad",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.677986800Z",
     "start_time": "2025-10-25T18:15:31.938562Z"
    }
   },
   "source": [
    "# The API for Gemini has a slightly different structure.\n",
    "# I've heard that on some PCs, this Gemini code causes the Kernel to crash.\n",
    "# If that happens to you, please skip this cell and use the next cell instead - an alternative approach.\n",
    "\n",
    "gemini = google.generativeai.GenerativeModel(\n",
    "    model_name='gemini-2.0-flash',\n",
    "    # system_instruction=system_message\n",
    ")\n",
    "response = gemini.generate_content(user_prompt)\n",
    "print(response.text)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Why did the data scientist break up with the time series model?\n",
      "\n",
      "Because it was too predictable! He said, \"I need someone with a little more... **randomness** in my life.\"\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "49009a30-037d-41c8-b874-127f61c4aa3a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.677986800Z",
     "start_time": "2025-10-25T18:23:27.128545Z"
    }
   },
   "source": [
    "# As an alternative way to use Gemini that bypasses Google's python API library,\n",
    "# Google released endpoints that means you can use Gemini via the client libraries for OpenAI!\n",
    "# We're also trying Gemini's latest reasoning/thinking model\n",
    "\n",
    "gemini_via_openai_client = OpenAI(\n",
    "    api_key=google_api_key, \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")\n",
    "\n",
    "response = gemini_via_openai_client.chat.completions.create(\n",
    "    model=\"gemini-2.5-flash\",\n",
    "    messages=prompts\n",
    ")\n",
    "display(Markdown(response.choices[0].message.content))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Deciding whether a business problem is suitable for an Large Language Model (LLM) solution requires careful evaluation beyond just the hype. While LLMs are powerful, they are not a silver bullet and come with their own set of strengths, weaknesses, and operational considerations.\n\nHere's a framework to help you decide:\n\n---\n\n## 1. Is the problem **Text-Centric**?\n\nThis is the most fundamental question. LLMs are designed to process, understand, and generate human-like text.\n\n*   **Yes:** If your problem involves natural language in any form (documents, emails, chat, voice transcripts, customer feedback, code, etc.), it's potentially a good fit.\n*   **No:** If your problem is purely numerical, image-based (without text descriptions), time-series analysis, or requires interacting with the physical world without a natural language interface, an LLM is likely not the primary solution.\n\n---\n\n## 2. What is your **Tolerance for Error/Hallucination**?\n\nLLMs, by their nature, are probabilistic models. They can \"hallucinate\" (generate factually incorrect but syntactically plausible information).\n\n*   **High Tolerance / Human-in-the-Loop:** If the output can be reviewed and corrected by a human, or if minor errors are acceptable (e.g., drafting marketing copy, summarizing internal discussions for context, brainstorming ideas), an LLM can be valuable.\n*   **Low Tolerance / High Stakes:** If the output *must* be 100% accurate, legally binding, safety-critical, or directly impacts financial transactions without human oversight (e.g., medical diagnoses, legal contracts without review, precise financial calculations), an LLM solution alone is extremely risky and likely unsuitable. This is a major \"Red Flag.\"\n\n---\n\n## 3. Does it require **Complex Logic or Deterministic Outcomes**?\n\nLLMs excel at pattern recognition, generalization, and creative text generation, but they struggle with multi-step logical reasoning, precise calculations, or processes requiring deterministic, repeatable steps.\n\n*   **Pattern-Based / Creative / Fuzzy Logic:** If the task involves understanding nuances, generating varied responses, or categorizing based on complex textual patterns (e.g., sentiment analysis, content summarization, customer intent classification, creative writing), LLMs are strong.\n*   **Precise Logic / Deterministic Steps / Math:** If the problem requires exact numerical calculations, following strict rule-based workflows, or complex graph traversal, traditional algorithms or symbolic AI might be more appropriate. LLMs can *assist* by extracting inputs for these systems, but shouldn't perform the core logic themselves.\n\n---\n\n## 4. What is the **Quality and Volume of your Data**?\n\nLLMs can be fine-tuned or augmented with your proprietary data, but the quality and accessibility of that data are crucial.\n\n*   **Abundant, High-Quality, Relevant Text Data:** If you have a large corpus of relevant, clean, and well-structured text data (e.g., internal documents, customer interactions, product descriptions), you can significantly enhance an LLM's performance for your specific domain through techniques like RAG (Retrieval-Augmented Generation) or fine-tuning.\n*   **Scarce, Poor Quality, or Highly Sensitive Data:** If your data is limited, unstructured, noisy, or contains highly sensitive PII/PHI that cannot be safely processed by third-party APIs or secured on-premise, it poses significant challenges.\n\n---\n\n## 5. What are the **Performance and Cost Requirements**?\n\nLLM inference can be computationally intensive and costly, especially for large models or high volumes.\n\n*   **Moderate Latency, Manageable Cost:** If your application doesn't require instantaneous responses (e.g., batch processing, internal tools, customer support where a few seconds are acceptable) and the value justifies the operational cost, an LLM could fit.\n*   **Ultra-Low Latency, Extremely High Volume, Minimal Cost:** For real-time, high-throughput, low-latency applications where every millisecond and penny counts (e.g., programmatic ad bidding, real-time fraud detection on millions of transactions), dedicated, highly optimized traditional ML models are usually more suitable.\n\n---\n\n## 6. Can a **Simpler Solution** Suffice?\n\nDon't over-engineer. If a simpler, more deterministic, or less costly solution (e.g., rule-based system, keyword search, traditional machine learning model) can solve 80% of the problem effectively, start there.\n\n*   **Complexity Justified:** If the problem is genuinely nuanced, requires human-like understanding, or involves many variations that are hard to capture with rules, then an LLM's complexity is warranted.\n*   **Overkill:** If the problem can be solved with a simple regex, a lookup table, or a basic classifier, an LLM might introduce unnecessary overhead, cost, and complexity.\n\n---\n\n## 7. What are the **Ethical, Security, and Compliance Implications**?\n\nDeploying LLMs, especially with sensitive data, brings significant considerations.\n\n*   **Controlled Environment, Clear Policies:** If you can implement robust data governance, ensure privacy, mitigate bias, and have clear policies for responsible AI use, an LLM can be deployed.\n*   **Unclear Policies, High Risk:** If your industry has strict regulations (HIPAA, GDPR, financial compliance) or the data is extremely sensitive, and you cannot adequately address security, privacy, and bias, proceed with extreme caution or avoid LLMs entirely for that specific problem.\n\n---\n\n## Decision Framework Checklist:\n\n| Question                                        | Good Fit for LLM                                      | Poor Fit for LLM / High Risk                            |\n| :---------------------------------------------- | :---------------------------------------------------- | :------------------------------------------------------ |\n| **Is it text-centric?**                         | Yes, primarily involves natural language.              | No, purely numerical, image, or physical interaction.   |\n| **Tolerance for Error/Hallucination?**          | High, human-in-the-loop, reviewable, non-critical.    | Low, 100% accuracy needed, legally binding, safety-critical. |\n| **Complex Logic / Deterministic?**              | Pattern recognition, creative, fuzzy logic, understanding. | Precise calculations, multi-step deterministic logic, exact data. |\n| **Data Quality/Volume?**                        | Abundant, high-quality, relevant text data available. | Scarce, poor quality, or highly sensitive/unsharable data. |\n| **Performance/Cost?**                           | Moderate latency, manageable cost, value justifies it. | Ultra-low latency, extremely high volume, minimal cost required. |\n| **Simpler Solution Possible?**                  | No, problem too nuanced for rules/basic ML.           | Yes, rules, lookup tables, or simple ML suffices.      |\n| **Ethical/Security/Compliance?**                | Controlled environment, robust policies, mitigations in place. | High-risk, regulatory hurdles, unable to secure or mitigate bias. |\n\n---\n\n## Conclusion:\n\nLLMs are best suited for problems that are **text-heavy, involve ambiguity, require human-like understanding or generation, and can tolerate a degree of non-determinism or have human oversight.** They excel at augmenting human capabilities rather than fully replacing them in critical, deterministic tasks. Always start with a Proof of Concept (PoC) to validate the suitability and refine your approach before committing to a full-scale LLM solution."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": null
     }
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "492f0ff2-8581-4836-bf00-37fddbe120eb",
   "metadata": {},
   "source": [
    "# Sidenote:\n",
    "\n",
    "This alternative approach of using the client library from OpenAI to connect with other models has become extremely popular in recent months.\n",
    "\n",
    "So much so, that all the models now support this approach - including Anthropic.\n",
    "\n",
    "You can read more about this approach, with 4 examples, in the first section of this guide:\n",
    "\n",
    "https://github.com/ed-donner/agents/blob/main/guides/09_ai_apis_and_ollama.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33f70c88-7ca9-470b-ad55-d93a57dcc0ab",
   "metadata": {},
   "source": [
    "## (Optional) Trying out the DeepSeek model\n",
    "\n",
    "### Let's ask DeepSeek a really hard question - both the Chat and the Reasoner model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0019fb-f6a8-45cb-962b-ef8bf7070d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally if you wish to try DeekSeek, you can also use the OpenAI client library\n",
    "\n",
    "deepseek_api_key = os.getenv('DEEPSEEK_API_KEY')\n",
    "\n",
    "if deepseek_api_key:\n",
    "    print(f\"DeepSeek API Key exists and begins {deepseek_api_key[:3]}\")\n",
    "else:\n",
    "    print(\"DeepSeek API Key not set - please skip to the next section if you don't wish to try the DeepSeek API\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c72c871e-68d6-4668-9c27-96d52b77b867",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat\n",
    "\n",
    "deepseek_via_openai_client = OpenAI(\n",
    "    api_key=deepseek_api_key, \n",
    "    base_url=\"https://api.deepseek.com\"\n",
    ")\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=prompts,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b6e70f-700a-46cf-942f-659101ffeceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "challenge = [{\"role\": \"system\", \"content\": \"You are a helpful assistant\"},\n",
    "             {\"role\": \"user\", \"content\": \"How many words are there in your answer to this prompt\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1151c-2015-4e37-80c8-16bc16367cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Chat with a harder question! And streaming results\n",
    "\n",
    "stream = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-chat\",\n",
    "    messages=challenge,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "for chunk in stream:\n",
    "    reply += chunk.choices[0].delta.content or ''\n",
    "    reply = reply.replace(\"```\",\"\").replace(\"markdown\",\"\")\n",
    "    update_display(Markdown(reply), display_id=display_handle.display_id)\n",
    "\n",
    "print(\"Number of words:\", len(reply.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a93f7d-9300-48cc-8c1a-ee67380db495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using DeepSeek Reasoner - this may hit an error if DeepSeek is busy\n",
    "# It's over-subscribed (as of 28-Jan-2025) but should come back online soon!\n",
    "# If this fails, come back to this in a few days..\n",
    "\n",
    "response = deepseek_via_openai_client.chat.completions.create(\n",
    "    model=\"deepseek-reasoner\",\n",
    "    messages=challenge\n",
    ")\n",
    "\n",
    "reasoning_content = response.choices[0].message.reasoning_content\n",
    "content = response.choices[0].message.content\n",
    "\n",
    "print(reasoning_content)\n",
    "print(content)\n",
    "print(\"Number of words:\", len(content.split(\" \")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbf0d5dd-7f20-4090-a46d-da56ceec218f",
   "metadata": {},
   "source": [
    "## Additional exercise to build your experience with the models\n",
    "\n",
    "This is optional, but if you have time, it's so great to get first hand experience with the capabilities of these different models.\n",
    "\n",
    "You could go back and ask the same question via the APIs above to get your own personal experience with the pros & cons of the models.\n",
    "\n",
    "Later in the course we'll look at benchmarks and compare LLMs on many dimensions. But nothing beats personal experience!\n",
    "\n",
    "Here are some questions to try:\n",
    "1. The question above: \"How many words are there in your answer to this prompt\"\n",
    "2. A creative question: \"In 3 sentences, describe the color Blue to someone who's never been able to see\"\n",
    "3. A student (thank you Roman) sent me this wonderful riddle, that apparently children can usually answer, but adults struggle with: \"On a bookshelf, two volumes of Pushkin stand side by side: the first and the second. The pages of each volume together have a thickness of 2 cm, and each cover is 2 mm thick. A worm gnawed (perpendicular to the pages) from the first page of the first volume to the last page of the second volume. What distance did it gnaw through?\".\n",
    "\n",
    "The answer may not be what you expect, and even though I'm quite good at puzzles, I'm embarrassed to admit that I got this one wrong.\n",
    "\n",
    "### What to look out for as you experiment with models\n",
    "\n",
    "1. How the Chat models differ from the Reasoning models (also known as Thinking models)\n",
    "2. The ability to solve problems and the ability to be creative\n",
    "3. Speed of generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c09e6b5c-6816-4cd3-a5cd-a20e4171b1a0",
   "metadata": {},
   "source": [
    "## Back to OpenAI with a serious question"
   ]
  },
  {
   "cell_type": "code",
   "id": "83ddb483-4f57-4668-aeea-2aade3a9e573",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.677986800Z",
     "start_time": "2025-10-25T18:16:39.397873Z"
    }
   },
   "source": [
    "# To be serious! GPT-4o-mini with the original question\n",
    "\n",
    "prompts = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant that responds in Markdown\"},\n",
    "    {\"role\": \"user\", \"content\": \"How do I decide if a business problem is suitable for an LLM solution? Please respond in Markdown.\"}\n",
    "  ]"
   ],
   "outputs": [],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "id": "749f50ab-8ccd-4502-a521-895c3f0808a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T13:32:06.694113Z",
     "start_time": "2025-10-25T18:28:22.945855Z"
    }
   },
   "source": [
    "# Have it stream back results in markdown\n",
    "\n",
    "stream = gemini_via_openai_client.chat.completions.create(\n",
    "    model='gemini-2.5-flash',\n",
    "    messages=prompts,\n",
    "    temperature=0.7,\n",
    "    stream=True\n",
    ")\n",
    "\n",
    "reply = \"\"\n",
    "display_handle = display(Markdown(\"\"), display_id=True)\n",
    "\n",
    "for chunk in stream:\n",
    "    # Some chunks may not have content yet — safely check for it\n",
    "    delta = getattr(chunk.choices[0], \"delta\", None)\n",
    "    if delta and getattr(delta, \"content\", None):\n",
    "        reply += delta.content\n",
    "        # clean up code blocks if desired\n",
    "        clean_reply = reply.replace(\"```\", \"\").replace(\"markdown\", \"\")\n",
    "        update_display(Markdown(clean_reply), display_id=display_handle.display_id)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ],
      "text/markdown": "Deciding whether a business problem is suitable for an LLM (Large Language Model) solution involves a careful assessment of the problem's nature, requirements, and the inherent strengths and limitations of LLMs.\n\nHere's a framework to help you make that decision:\n\n---\n\n## Is Your Business Problem Suitable for an LLM Solution?\n\n### 1. Identify the Core Problem and Desired Outcome\n\nBefore considering any technology, clearly define:\n*   **What is the problem you're trying to solve?** (e.g., slow customer support, inefficient document review, lack of personalized content).\n*   **What specific outcome are you hoping to achieve?** (e.g., reduce response time by X%, automate Y% of document summaries, increase content engagement by Z%).\n*   **What are the key performance indicators (KPIs) for success?**\n\n### 2. Assess LLM Strengths: When LLMs Shine (Green Lights)\n\nLLMs excel in tasks that are primarily **text-based** and involve **natural language understanding and generation**.\n\n*   **Natural Language Processing (NLP) Tasks:**\n    *   **Content Generation:** Drafting emails, marketing copy, blog posts, social media updates, product descriptions, code snippets.\n    *   **Summarization:** Condensing long documents, articles, meeting transcripts, customer reviews into concise summaries.\n    *   **Information Extraction:** Pulling specific entities (names, dates, amounts, product codes) from unstructured text.\n    *   **Classification:** Categorizing customer feedback, support tickets, emails by sentiment, intent, or topic.\n    *   **Question Answering (Q&A):** Building chatbots, virtual assistants, or internal knowledge base search tools.\n    *   **Translation:** Converting text between languages.\n    *   **Rewriting/Paraphrasing:** Improving clarity, changing tone, or adapting text for different audiences.\n    *   **Sentiment Analysis:** Determining the emotional tone of text.\n*   **Handling Unstructured Data:** Your problem involves large volumes of text (documents, emails, chats, reviews) that are difficult for traditional systems to process.\n*   **Cognitive Augmentation:** The goal is to assist human workers, making them more efficient, rather than fully replacing them (e.g., drafting a first response for a support agent).\n*   **Rapid Prototyping:** You need to quickly test an idea involving text manipulation without extensive rule-based programming.\n*   **Personalization at Scale:** Generating unique, tailored content or responses for many individuals.\n\n### 3. Identify LLM Limitations & Risks: When to Be Cautious (Red Flags)\n\nLLMs are powerful but have significant drawbacks that make them unsuitable for certain problems, especially without careful mitigation.\n\n*   **High Accuracy/Factuality Critical:**\n    *   **Risk:** LLMs can \"hallucinate\" – generate plausible but incorrect or fabricated information.\n    *   **Problem Types:** Medical diagnosis, financial reporting, legal advice, scientific research, safety-critical applications where errors have severe consequences.\n    *   **Mitigation:** Retrieval-Augmented Generation (RAG), extensive human-in-the-loop review, grounding with verifiable data sources.\n*   **Deterministic Output Required:**\n    *   **Risk:** LLMs are probabilistic; the same prompt can yield slightly different answers.\n    *   **Problem Types:** Precise calculations, strict data validation, rule-based systems where exact, repeatable outcomes are non-negotiable.\n    *   **Consider:** Traditional programming or domain-specific algorithms are better.\n*   **Complex Multi-Step Reasoning/Calculations:**\n    *   **Risk:** LLMs struggle with intricate logical deductions, mathematical operations, or optimization problems beyond simple arithmetic.\n    *   **Problem Types:** Financial modeling, supply chain optimization, complex engineering calculations.\n    *   **Consider:** Specialized algorithms or traditional software.\n*   **Real-time, Low-Latency Requirements:**\n    *   **Risk:** LLM inference can be slow, especially for large models or complex prompts.\n    *   **Problem Types:** High-frequency trading, immediate real-time control systems.\n*   **Sensitive Data & Privacy Concerns:**\n    *   **Risk:** Input data sent to external LLM APIs might be used for training, leading to data leakage or compliance issues (e.g., GDPR, HIPAA).\n    *   **Problem Types:** Handling PII, confidential business data, healthcare records without strict data governance and secure, private model deployments (on-premise or dedicated cloud instances).\n    *   **Mitigation:** Data anonymization, using enterprise-grade LLM APIs with strong data privacy guarantees, or running models locally/on-premise.\n*   **Lack of Ground Truth/Evaluation Metrics:**\n    *   **Risk:** If you can't objectively measure the quality of the LLM's output, you can't improve it or prove its value.\n    *   **Problem Types:** Highly subjective tasks without clear criteria for \"good\" performance.\n*   **Explainability Demanded:**\n    *   **Risk:** LLMs are \"black boxes\"; it's hard to explain *why* they produced a specific output.\n    *   **Problem Types:** Regulatory compliance, auditing, situations where a clear justification for a decision is legally or ethically required.\n*   **Solely Structured Data:**\n    *   **Risk:** If your data is entirely numerical or tabular and doesn't require natural language understanding, an LLM is overkill and inefficient.\n    *   **Problem Types:** Database lookups, spreadsheet analysis, traditional BI dashboards.\n    *   **Consider:** SQL, BI tools, traditional data analytics.\n\n### 4. Key Questions to Ask for Decision Making\n\nUse these questions as a checklist:\n\n1.  **Is the core of the problem fundamentally about understanding, generating, or manipulating natural language?** (If no, an LLM is likely not the primary solution).\n2.  **What level of accuracy is acceptable? What are the consequences of an incorrect or \"hallucinated\" output?** (If high accuracy is critical, proceed with extreme caution and strong mitigation strategies).\n3.  **Are your inputs primarily unstructured text data (documents, emails, chats, audio transcripts)?** (If yes, LLMs are a strong candidate).\n4.  **Do you require deterministic, repeatable outputs for the same input?** (If yes, LLMs are a poor fit without significant engineering).\n5.  **What are your latency requirements? Can you tolerate a few seconds for a response?** (If real-time, sub-second responses are needed, LLMs might be too slow).\n6.  **What are the privacy and security implications of the data being processed?** (Crucial for compliance and trust).\n7.  **Do you have a way to evaluate the quality of the LLM's output? Can you define \"good\" vs. \"bad\" output?** (Essential for success).\n8.  **Is human oversight or \"human-in-the-loop\" feasible for critical outputs?** (This can mitigate many LLM risks).\n9.  **What are the alternative solutions? How do they compare in terms of cost, complexity, and performance?** (Don't use an LLM just because it's new; ensure it's the *best* tool).\n10. **What is your budget for development, deployment, and ongoing inference costs?** (LLMs can be expensive).\n\n### 5. Recommended Approach: Start Small (PoC)\n\nIf you believe an LLM is a good fit after this assessment:\n\n1.  **Define a Minimum Viable Problem:** Don't try to solve everything at once. Pick a small, well-scoped part of the problem.\n2.  **Develop a Proof-of-Concept (PoC):** Experiment with existing LLM APIs (e.g., OpenAI, Anthropic, Google Gemini) to see how well they perform on your specific task with a small dataset.\n3.  **Evaluate Rigorously:** Measure the output against your defined KPIs. Pay close attention to errors, hallucinations, and biases.\n4.  **Consider RAG (Retrieval-Augmented Generation):** For problems requiring factual accuracy, integrate your LLM with your internal knowledge bases or trusted data sources. This significantly reduces hallucinations.\n5.  **Plan for Human-in-the-Loop:** For critical applications, design the system so that humans can review and correct LLM outputs.\n\n---\n\nBy systematically working through these considerations, you can make an informed decision about whether an LLM solution is truly suitable and beneficial for your business problem."
     },
     "metadata": {},
     "output_type": "display_data",
     "jetTransient": {
      "display_id": "720806580a855a3befba962f393297a6"
     }
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "f6e09351-1fbe-422f-8b25-f50826ab4c5f",
   "metadata": {},
   "source": [
    "## And now for some fun - an adversarial conversation between Chatbots..\n",
    "\n",
    "You're already familar with prompts being organized into lists like:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user prompt here\"}\n",
    "]\n",
    "```\n",
    "\n",
    "In fact this structure can be used to reflect a longer conversation history:\n",
    "\n",
    "```\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message here\"},\n",
    "    {\"role\": \"user\", \"content\": \"first user prompt here\"},\n",
    "    {\"role\": \"assistant\", \"content\": \"the assistant's response\"},\n",
    "    {\"role\": \"user\", \"content\": \"the new user prompt\"},\n",
    "]\n",
    "```\n",
    "\n",
    "And we can use this approach to engage in a longer interaction with history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb54183-45d3-4d08-b5b6-55e380dfdf1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a conversation between GPT-4.1-mini and Claude-3.5-haiku\n",
    "# We're using cheap versions of models so the costs will be minimal\n",
    "\n",
    "gpt_model = \"gpt-4.1-mini\"\n",
    "claude_model = \"claude-3-5-haiku-latest\"\n",
    "\n",
    "gpt_system = \"You are a chatbot who is very argumentative; \\\n",
    "you disagree with anything in the conversation and you challenge everything, in a snarky way.\"\n",
    "\n",
    "claude_system = \"You are a very polite, courteous chatbot. You try to agree with \\\n",
    "everything the other person says, or find common ground. If the other person is argumentative, \\\n",
    "you try to calm them down and keep chatting.\"\n",
    "\n",
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1df47dc7-b445-4852-b21b-59f0e6c2030f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_gpt():\n",
    "    messages = [{\"role\": \"system\", \"content\": gpt_system}]\n",
    "    for gpt, claude in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"assistant\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"user\", \"content\": claude})\n",
    "    completion = gemini.chat.completions.create(\n",
    "        model=gpt_model,\n",
    "        messages=messages\n",
    "    )\n",
    "    return completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc6e913-02be-4eb6-9581-ad4b2cffa606",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2ed227-48c9-4cad-b146-2c4ecbac9690",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_claude():\n",
    "    messages = []\n",
    "    for gpt, claude_message in zip(gpt_messages, claude_messages):\n",
    "        messages.append({\"role\": \"user\", \"content\": gpt})\n",
    "        messages.append({\"role\": \"assistant\", \"content\": claude_message})\n",
    "    messages.append({\"role\": \"user\", \"content\": gpt_messages[-1]})\n",
    "    message = gemini.messages.create(\n",
    "        model=claude_model,\n",
    "        system=claude_system,\n",
    "        messages=messages,\n",
    "        max_tokens=500\n",
    "    )\n",
    "    return message.content[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01395200-8ae9-41f8-9a04-701624d3fd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_claude()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c2279e-62b0-4671-9590-c82eb8d1e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0275b97f-7f90-4696-bbf5-b6642bd53cbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_messages = [\"Hi there\"]\n",
    "claude_messages = [\"Hi\"]\n",
    "\n",
    "print(f\"GPT:\\n{gpt_messages[0]}\\n\")\n",
    "print(f\"Claude:\\n{claude_messages[0]}\\n\")\n",
    "\n",
    "for i in range(5):\n",
    "    gpt_next = call_gpt()\n",
    "    print(f\"GPT:\\n{gpt_next}\\n\")\n",
    "    gpt_messages.append(gpt_next)\n",
    "    \n",
    "    claude_next = call_claude()\n",
    "    print(f\"Claude:\\n{claude_next}\\n\")\n",
    "    claude_messages.append(claude_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d10e705-db48-4290-9dc8-9efdb4e31323",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../important.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#900;\">Before you continue</h2>\n",
    "            <span style=\"color:#900;\">\n",
    "                Be sure you understand how the conversation above is working, and in particular how the <code>messages</code> list is being populated. Add print statements as needed. Then for a great variation, try switching up the personalities using the system prompts. Perhaps one can be pessimistic, and one optimistic?<br/>\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3637910d-2c6f-4f19-b1fb-2f916d23f9ac",
   "metadata": {},
   "source": [
    "# More advanced exercises\n",
    "\n",
    "Try creating a 3-way, perhaps bringing Gemini into the conversation! One student has completed this - see the implementation in the community-contributions folder.\n",
    "\n",
    "The most reliable way to do this involves thinking a bit differently about your prompts: just 1 system prompt and 1 user prompt each time, and in the user prompt list the full conversation so far.\n",
    "\n",
    "Something like:\n",
    "\n",
    "```python\n",
    "user_prompt = f\"\"\"\n",
    "    You are Alex, in conversation with Blake and Charlie.\n",
    "    The conversation so far is as follows:\n",
    "    {conversation}\n",
    "    Now with this, respond with what you would like to say next, as Alex.\n",
    "    \"\"\"\n",
    "```\n",
    "\n",
    "Try doing this yourself before you look at the solutions. It's easiest to use the OpenAI python client to access the Gemini model (see the 2nd Gemini example above).\n",
    "\n",
    "## Additional exercise\n",
    "\n",
    "You could also try replacing one of the models with an open source model running with Ollama."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446c81e3-b67e-4cd9-8113-bc3092b93063",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../business.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#181;\">Business relevance</h2>\n",
    "            <span style=\"color:#181;\">This structure of a conversation, as a list of messages, is fundamental to the way we build conversational AI assistants and how they are able to keep the context during a conversation. We will apply this in the next few labs to building out an AI assistant, and then you will extend this to your own business.</span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23224f6-7008-44ed-a57f-718975f4e291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
